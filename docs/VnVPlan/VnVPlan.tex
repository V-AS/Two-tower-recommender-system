\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}
\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Feb. 24, 2025 & 1.0 & First draft - Unit tests will be added after the MIS has been completed..\\
Feb. 25, 2025 & 1.1 & Minor updates after VnV presentation\\
Mar. 27, 2025 & 1.2 & Addressed comments from Dr. Smith.\\
Apr. 2, 2025 & 1.3 & Added unit tests.\\
Apr. 11 2025 & 2.0 & Revision 1\\
\bottomrule
\end{tabularx}

~\\

\newpage

\tableofcontents

\listoftables

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  R & Requirement\\
  NFR & Nonfunctional Requirement\\
  TTE & Two Tower Embedding\\
  RecSys & Recommendation System\\
  ANN & Approximate Nearest Neighbor\\
  SRS & Software Requirements Specification\\
  FAISS & Facebook AI Similarity Search\\
  DNN & Deep Neural Network\\
  \bottomrule
\end{tabular}\\

\newpage

\pagenumbering{arabic}
\nocite{*}
This document outlines the verification and validation plan for the Two-Tower Embeddings Recommendation System (TTE RecSys) to ensure compliance with the requirements and objectives specified in the \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{Software Requirements Specification (SRS)}. It is structured to first present general information and verification strategies, followed by detailed descriptions of system and unit testing for both functional and non-functional requirements.

\section{General Information}

\subsection{Summary}

The software under test is the Two-Tower Embedding Recommendation System, which generates personalized recommendations using user and item embeddings. The system consists of two main components:
\begin{itemize}
  \item Training Phase: Learns user and item embedding functions using a deep neural network architecture, optimized via Stochastic Gradient Descent (SGD).
  \item Inference Phase: Retrieves candidate items using Approximate Nearest Neighbor (ANN) search and ranks them by dot product similarity.
\end{itemize}

The system is implemented in Python, leveraging libraries such as PyTorch for model training and FAISS for ANN search.

\subsection{Objectives}
The primary objectives of this VnV plan are:
\begin{itemize}
  \item Correctness: Verify that the system correctly implements the mathematical models for training (e.g., MSE loss) and inference (e.g., ANN search, dot product ranking).
  \item Scalability: Demonstrate that the system can support incremental update when new data available.
\end{itemize}
\noindent Out-of-Scope Objectives

\begin{itemize}
  \item External Library Verification: Libraries such as PyTorch and FAISS are assumed to be correct and are not verified as part of this plan.
\end{itemize}

\subsection{Challenge Level and Extras}
This is a non-research project. The extra component of this project will be a user manual.

\subsection{Relevant Documentation}
The following documents are available for this project:
\href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{Software Requirements Specification}, \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Design/SoftArchitecture/MG.pdf}{Module Guide}, and \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{Module Interface Specification} 

\section{Plan}

The VnV plan starts with an introduction to the verification and validation team, followed by verification plans for the SRS and design. Next, it covers verification plans for the VnV Plan and implementation. Finally, it includes sections on automated testing and verification tools as well as the software validation plan .

\subsection{Verification and Validation Team}\label{VnVT}

\begin{table}[h]
  \centering
  \resizebox{\textwidth}{!}{ %
      \begin{tabular}{ |l|l|p{2cm}|p{5cm}| } 
  \hline        
  
     Name & Document & Role & Description \\
  \hline
    Yinying Huo & All & Author & Prepare all documentation, develop the software, and validate the implementation accoridng to the VnV plan. \\ \hline
    Dr. Spencer Smith & All & Instructor/ Reviewer & Review all the documents.  \\ \hline
    Yuanqi Xue & All & Domain Expert & Review all the documents.  \\ \hline     	  
  \end{tabular} %
  }
  \caption{Verification and Validation Team}
  \label{Table:VnVT}
  \end{table}
  
\subsection{SRS Verification Plan}

The Software Requirements Specification (SRS) will be reviewed by domain expert Yuanqi Xue and Dr. Smith. Feedback from reviewers will be provided on GitHub, and the author will need to address it.

\noindent There is a \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/SRS-Checklist.pdf}{SRS checklist} designed by Dr. Spencer Smith available to use.

\subsection{Design Verification Plan}

The design verification, including the Module Guide (MG) and Module Interface Specification (MIS), will be reviewed by domain expert Yuanqi Xue and Dr. Smith. Feedback from reviewers will be provided on GitHub, and the author will need to address it.

\noindent Dr. Spencer Smith has created a \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/MG-Checklist.pdf}{MG checklist} and \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/MIS-Checklist.pdf}{MIS checklist}, both of which are available for use.

\subsection{Verification and Validation Plan Verification Plan}

The Verification and Validation (VnV) Plan will be reviewed by domain expert Yuanqi Xue and Dr. Smith. Feedback from reviewers will be provided on GitHub, and the author will need to address it.

\noindent  There is a \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/VnV-Checklist.pdf}{VnV checklist} designed by Dr. Spencer Smith available to use.
%

\subsection{Implementation Verification Plan}

The implementation will be verified by testing both the functional and non-functional requirements outlined in section \ref{SystemTest}. Unit tests, as described in section \ref{UnitTest}, will also be performed. Additionally, a code walkthrough will be conducted with the class during the final presentation.

\subsection{Automated Testing and Verification Tools}
All system tests and unit tests will be performed using Python scripts.  
GitHub Actions is used for continuous integration, and the workflow will run all unit tests.
Pytest will be used to generate the code coverage reports.

\subsection{Software Validation Plan}

The software validation plan is beyond the scope of \progname, as it requires additional time and data that are not available within the scope of the project.

\section{System Tests}\label{SystemTest}

This section covers the system tests that will be applied to both the functional and non-functional requirements.

\subsection{Tests for Functional Requirements}

The functional requirements are tested in the following areas: input validation, model training convergence, and model storage. These tests ensure that the system behaves as expected under various conditions.

\subsubsection{Area of Testing 1: Dataset}

\begin{enumerate}

  \item{test-id1\\}
  Control: Automatic\\
  Initial State: Before training the DNN.\\
  Input: A CSV file (data/processed/recommender\_data.csv) containing user-item interactions with expected columns: `User-ID', `Book-Rating', `Book-Title', `Book-Author', `Year-Of-Publication', `Publisher', and `Age'.\\
  Output: A boolean value: True if the dataset meets all validation criteria (contains required columns and each user-item pair has an associated reward), False otherwise.\\
  Test Case Derivation: Verifies R1 which requires that all user-item pairs in the training dataset should have an associated reward as specified in the \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{SRS}.

  How test will be performed: This test will be performed automatically using GitHub Actions via \texttt{test\_data\_validation.py}, which runs before model training to verify the dataset meets all requirements.
					
\end{enumerate}


\subsubsection{Area of Testing 2: Model Training Convergence}

\begin{enumerate}
  \item{test-id2\\}

  Control: Automatic\\
  Initial State: After training of the DNN\\
  Input: The training\_history.json file containing loss values recorded during training\\
  Output: A boolean value: True if the loss decreases over time (final loss $\leq$ initial loss), False otherwise.\\
  Test Case Derivation: Verifies R2 which requires successful training of embedding functions as specified in the \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{SRS}.\\
  How the test will be performed: This test will be performed automatically using GitHub Actions via \texttt{test\_model\_convergence.py}, which analyzes the training history after model training completes.

\end{enumerate}

\subsubsection{Area of Testing 3: Model Storage}

\begin{enumerate}
  \item{test-id3:\\}
  
  Control: Automatic\\
  Initial State: After model training is complete\\
  Input: Path to output directory containing trained models\\
  Output: A boolean value: True if all expected files exist and can be loaded with proper structure, False otherwise.\\
  Test Case Derivation: Verifies R3 which requires proper model storage and persistence.\\
  How test will be performed: The test will be performed automatically using GitHub Actions via \texttt{test\_model\_storage.py}, which checks for the existence and loadability of all output files.
\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

\subsubsection{Reliability}

The reliability of the software is tested through the tests for
functional requirements in section 4.1 and 5.2.

\subsubsection{Portability}

\begin{enumerate}

\item{test-id4\\}

Type: Automatic and manual
					
Initial State: None
					
Input/Condition: None
					
Output/Result: The results of all automatic tests and feedback from users.

How test will be performed: All automatic tests will be conducted during the continuous integration workflow. Potential users will install the project on their computers (Windows, macOS, or Linux) and follow the instructions in \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/README.md}{README.md} to run the software. 
\end{enumerate}
\subsubsection{Usability}
\begin{enumerate}

  \item{test-id5\\}
  
  Type: Manual
  
  Initial State: The software is setup and ready to use.
  
  Input/Condition: None
  
  Output/Result: Survey result from the user
  
  How test will be performed: The user will be asked to fill out the survey after using this software. The survey can be found in appendix \ref{US}.
  
  
  \end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

The table \ref{Table:S_trace}
 shows the traceability between test cases and requirements

\begin{table}[h!]\label{Table:S_trace}
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
    & test-id1& test-id2& test-id3&test-id4&test-id5 \\
  \hline
  R1        & X&  & & &  \\ \hline
  R2        & & X& & &  \\ \hline
  R3        & & & X& & \\ \hline
  R4        & & X& & & \\ \hline
  R5        & &X & & & \\ \hline
  R6        & &X & & & X\\ \hline
  NFR1      & & & & &  X\\ \hline
  NFR2      &X & X& X& & \\ \hline
  NFR3      & & & &X & \\ \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between Test Cases and Requirements}
  \label{Table:A_trace}
  \end{table}

\section{Unit Test Description}\label{UnitTest}

The unit tests for this system will follow a hierarchical approach based on the module decomposition in the Module Guide. The testing philosophy focuses on:

1. Black-box testing of module interfaces according to their specifications
2. White-box testing for complex algorithms and edge cases
3. Mock objects for isolating modules from their dependencies

\subsection{Unit Testing Scope}

The unit testing will focus on the modules with direct user interaction and core algorithmic functionality. Some modules will be tested indirectly through system tests or as part of the testing of other modules. The following modules will be tested directly with unit tests:
\begin{enumerate}
  \item Data Processing Module (M2)
  \item Embedding Generation Module (M4)
  \item Neural Network Architecture Module (M6)
  \item ANN Search Module (M7)
  \item Vector Operations Module (M8)
\end{enumerate}


The following modules are not directly tested with unit tests but are verified through system tests or user interaction:

\noindent\textbf{System Interface Module(M1)}\\
The System Interface module is tested indirectly through:
\begin{itemize}
    \item System test-id3 (Model Storage), which verifies the file I/O capabilities of this module
    \item Usage within other modules, which depend on its functionality for loading and saving models and embeddings
\end{itemize}

This approach is sufficient because the module contains straightforward file I/O operations with well-defined error handling.

\noindent\textbf{Model Training Module (M3)}\\
The Model Training module is tested indirectly through:
\begin{itemize}
    \item System test-id2 (Model Training Convergence), which verifies the training process produces properly converging loss values
\end{itemize}


\noindent\textbf{Recommendation Module (M5)}\\
The Recommendation module is tested indirectly through:
\begin{itemize}
    \item Usability testing (test-id5), where users evaluate the quality of recommendations
\end{itemize}

This module integrates the outputs of other tested modules (embedding generation and ANN search), so thorough testing of those components provides confidence in this module's correct operation.


\subsection{Tests for Functional Requirements}

\subsubsection{Data Processing Module (M2)}

\begin{enumerate}
  \item{test-M2-1: Dataset Validation Test\\}
  Type: Automatic, Functional
            
  Initial State: None
            
  Input: Path to CSV file
            
  Output: Boolean value (True if the dataset meets all validation criteria, False otherwise)

  Test Case Derivation: R1 requires the system to accept valid input data with each user-item pair having an associated reward.
  
  How test will be performed: 
  Load the dataset using \texttt{load\_data} method, then apply \texttt{validate\_data} method to verify the dataset contains all required columns.


  \item{test-M2-2 Data Loading Test\\}
  Type: Automatic, Functional

  Initial State: None

  Input: Path to a test CSV file

  Output: Pandas DataFrame containing loaded data with expected columns

  Test Case Derivation: The system needs to correctly load data from CSV files.

  How test will be performed: Call \texttt{load\_data} with a test CSV file path and verify the DataFrame shape and column names match expected values.

  \item{test-M2-3 Data Preprocessing Test\\}
  Type: Automatic, Functional

  Initial State: None

  Input: Pandas DataFrame with raw user-book interaction data

  Output: Processed Pandas DataFrame with additional derived features

  Test Case Derivation: R1 requires the system to preprocess data for model training.

  How test will be performed: Pass a sample DataFrame to \texttt{preprocess\_data} and verify all expected derived features are created, including 'Author-Frequency', 'Publisher-Frequency', 'Age-Normalized', etc.

  \item{test-M2-4: Missing Value Handling Test\\}
  
  Type: Automatic, Functional

  Initial State: None

  Input: Pandas DataFrame with missing values in various columns

  Output: Processed DataFrame with no NaN values

  Test Case Derivation: The system must handle incomplete data gracefully for production usage.

  How test will be performed: Create a DataFrame with deliberately missing values in `Age', `State', and `Country' columns, process it with \texttt{preprocess\_data}, and verify all NaN values are replaced with appropriate defaults.
  \item{test-M2-5: Training Data Creation Test\\}
  Type: Automatic, Functional

  Initial State: None

  Input: Pandas DataFrame with all required derived features

  Output: Dictionary where keys are strings `user\_ids', `item\_ids', `ratings', `user\_features', and `item\_features', with values being arrays of user identifiers, arrays of item identifiers, arrays of numerical rating values, lists of User Feature dictionaries, and lists of Item Feature dictionaries, respectively

  Test Case Derivation: The system needs to convert DataFrame to a format suitable for neural network training.

  How test will be performed: Pass a processed sample DataFrame to \texttt{create\_training\_data} and verify the returned dictionary contains all required keys with arrays of correct shapes.
\end{enumerate}


\subsubsection{Embedding Generation Module (M4)}
\begin{enumerate}

  \item{test-M4-1: User Embedding Generation Test\\}
  Type: Automatic, Functional
            
  Initial State: Initialized EmbeddingGenerator with trained models
            
  Input: NumPy array of user feature
            
  Output: NumPy array of user embedding vectors
  
  Test Case Derivation: R4 requires the system to generate user embeddings based on user features.
  
  How test will be performed: Generate embeddings for sample user features and verify embedding dimension of each embedding vector.

  \item{test-M4-2: Item Embedding Generation Test\\}
  
  Type: Automatic, Functional

  Initial State: Initialized EmbeddingGenerator with trained models

  Input: NumPy array of item feature vectors

  Output: NumPy array of item embedding vectors

  Test Case Derivation: R4 requires the system to generate embeddings based on item features.

  How test will be performed: Generate embeddings for sample item features and verify embedding dimension of each embedding vector.

  \item{test-M4-3: Embedding Generator Initialization Test}
  Type: Automatic, Functional

  Initial State: None

  Input: Trained DNN for users and items

  Output: Initialized EmbeddingGenerator instance

  Test Case Derivation: The system must initialize embedding generators with compatible models that produce embeddings of the same dimension.

  How test will be performed: Initialize the generator with models of compatible and incompatible dimensions, verify successful initialization for compatible models and error raising for incompatible ones.
  \end{enumerate}

\subsubsection{Neural Network Architecture Module (M6)}

\begin{enumerate}

  \item{test-M6-1: Network Architecture Test\\}
  Type: Automatic, Functional
            
  Initial State: None
            
  Input: input\_dim (int), embedding\_dim (int)
            
  Output: DNN with the expected layer structure
  
  Test Case Derivation: R2 requires the system to create models for embedding generation with appropriate architecture.
  
  How test will be performed: Create a DNN with specified parameters and verify the network structure (input size, hidden layer size, output size) matches the expected configuration.
  
  \item{test-M6-2: Forward Pass Test\\}

  Type: Automatic, Functional
  
  Initial State: Initialized DNN instance
  
  Input: PyTorch tensor of feature vectors
  
  Output: PyTorch tensor of embedding vectors
  
  Test Case Derivation: The network must produce properly normalized embeddings for similarity calculations.
  
  How test will be performed: Run a forward pass on a batch of sample data and verify output dimensions and L2-normalization property of each embedding.
  
  \item{test-M6-3: Network Initialization Test}

  Type: Automatic, Functional
  
  Initial State: None
  
  Input: input\_dim (int), embedding\_dim (int)
  
  Output: DNN with properly initialized weights (non-zero)
  
  Test Case Derivation: Proper weight initialization is essential for training convergence and embedding diversity.
  
  How test will be performed: Create a network and verify weights are non-zero.
  
  \item{test-M6-4: Batch Processing Test\\}

  Type: Automatic, Functional
  
  Initial State: Initialized DNN in evaluation mode
  
  Input: Single feature vector and batch of identical feature vectors
  
  Output: Identical embeddings for both inputs (when in evaluation mode)
  
  Test Case Derivation: Batch processing should produce the same embeddings as individual processing for the same inputs when not in training mode.
  
  How test will be performed: Compare embeddings generated by batch processing with those from individual processing, verifying consistent outputs in evaluation mode.
  

\end{enumerate}
\subsubsection{ANN Search Module (M7)}

\begin{enumerate}

  \item{test-M7-1: Exact Match Search Test\\}
  Type: Automatic, Functional
            
  Initial State: None
            
  Input: Array of item embeddings and a query embedding identical to one of the items in the array
            
  Output: Search results containing the exact matching item ID at the top position
  
  Test Case Derivation: R6 requires efficient retrieval of nearest items, including exact matches.
  
  How test will be performed: Build an ANN index with item embeddings including a known vector, search using that same vector as query, and verify the matching item is returned as the top result.
  
  \item{test-M7-2: Approximate Match Search Test\\}
  Type: Automatic, Functional
  
  Initial State: None
  
  Input: Array of item embeddings and a query embedding similar but not identical to one of the items in the array
  
  Output: Search results containing the most similar item ID among the top results
  
  Test Case Derivation: R6 requires the system to find similar items even without exact matches.
  
  How test will be performed: Build an index, search with a query vector that is close to (but not identical to) a vector in the index, and verify the results include the expected similar item.
  
  \item{test-M7-3: Index Save/Load Test}
  Type: Automatic, Functional
  
  Initial State: None
  
  Input: ANN index object and path to save/load location
  
  Output: All load and save operations run successfully.
  
  Test Case Derivation: R3 requires model and index storage and loading.
  
  How test will be performed: Save an index to disk, load it back, and verify the loaded index produces the same search results for identical queries.
  \item{test-M7-4: Multiple Results Test\\}
  Type: Automatic, Functional
  
  Initial State: None
  
  Input: Array of item embeddings, query embedding, and number of results to return (k)
  
  Output: Array of exactly k (item\_id, similarity\_score) tuples sorted by descending similarity
  
  Test Case Derivation: R6 requires the system to return a customizable number of recommendations.
  
  How test will be performed: Build an index with multiple vectors, search with a query requesting k results, and verify exactly k results are returned in descending order of similarity.
  \end{enumerate}

\subsubsection{Vector Operations Module (M8)}

\begin{enumerate}

  \item{test-M8-1: Dot Product Calculation Test\\}
  Type: Automatic, Functional
            
  Initial State: None
            
  Input: Two vectors (NumPy arrays or lists) of the same dimension
            
  Output: Scalar dot product value calculated as the sum of element-wise products
  
  Test Case Derivation: R5 requires consistent ranking based on dot product similarity.
  
  How test will be performed: Calculate the dot product of various vector pairs (identical, orthogonal, similar, opposite direction) and verify the results match expected values.

\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

Unit testing the nonfunctional requirements is beyond the scope. 

The nonfunctional requirements will be tested primarily through the usability survey.


\subsection{Traceability Between Test Cases and Modules}
The table \ref{Table:Unit} shows the traceability between test cases and modules.


\begin{table}[h!]\label{Table:Unit}
  \centering
  \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
  \hline
  \textbf{Test ID} & \textbf{M1} & \textbf{M2} & \textbf{M3} & \textbf{M4} & \textbf{M5} & \textbf{M6} & \textbf{M7} & \textbf{M8} \\
  \hline
  test-M2-1 &   & X &   &   &   &   &   &   \\
  test-M2-2 &   & X &   &   &   &   &   &   \\
  test-M2-3 &   & X &   &   &   &   &   &   \\
  test-M2-4 &   & X &   &   &   &   &   &   \\
  test-M2-5 &   & X &   &   &   &   &   &   \\
  test-M4-1 &   &   &   & X &   &   &   &   \\
  test-M4-2 &   &   &   & X &   &   &   &   \\
  test-M4-3 &   &   &   & X &   &   &   &   \\
  test-M6-1 &   &   &   &   &   & X &   &   \\
  test-M6-2 &   &   &   &   &   & X &   &   \\
  test-M6-3 &   &   &   &   &   & X &   &   \\
  test-M6-4 &   &   &   &   &   & X &   &   \\
  test-M7-1 &   &   &   &   &   &   & X &   \\
  test-M7-2 &   &   &   &   &   &   & X &   \\
  test-M7-3 &   &   &   &   &   &   & X &   \\
  test-M7-4 &   &   &   &   &   &   & X &   \\
  test-M8-1 &   &   &   &   &   &   &   & X \\
  \hline
  \end{tabular}
  \caption{Traceability Matrix Between Test Cases and Modules}
  \end{table}

  \bibliographystyle {plainnat}
  \bibliography {../../refs/References}

\newpage
\section{Appendix}


\subsection{Usability Survey}\label{US}
\begin{itemize}
\item On a scale from 1 to 5, with 5 being the most satisfied, how would you rate your overall experience using the software?
\item Were the installation and setup process straightforward? If not, what difficulties did you encounter?
\item Did you find the user interface intuitive and easy to navigate? If not, what improvements would you suggest?
\item Did the software meet your expectations? If not, what features or improvements would you like to see?
\end{itemize}

\end{document}