\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Feb 24, 2025 & 1.0 & First draft – Unit tests will be added later.\\
\bottomrule
\end{tabularx}

~\\

\newpage

\tableofcontents

\listoftables

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  R & Requirement\\
  NFR & Nonfunctional Requirement\\
  TTE & Two Tower Embedding\\
  RecSys & Recommendation System\\
  ANN & Approximate Nearest Neighbor\\
  SRS & Software Requirements Specification\\
  FAISS & Facebook AI Similarity Search\\
  \bottomrule
\end{tabular}\\

\newpage

\pagenumbering{arabic}

This document outlines the verification and validation plan for the Two-Tower Embeddings Recommendation System (TTE RecSys) to ensure compliance with the requirements and objectives specified in the Software Requirements Specification (SRS). It is structured to first present general information and verification strategies, followed by detailed descriptions of system and unit testing for both functional and non-functional requirements.

\section{General Information}

\subsection{Summary}

The software under test is a Two-Tower Embedding Recommendation System, which generates personalized recommendations using user and item embeddings. The system consists of two main components:
\begin{itemize}
  \item Training Phase: Learns user and item embedding functions using a deep neural network architecture, optimized via stochastic gradient descent (SGD).
  \item Inference Phase: Retrieves candidate items using Approximate Nearest Neighbor (ANN) search and ranks them by dot product similarity.
\end{itemize}

The system is implemented in Python, leveraging libraries such as PyTorch for model training and FAISS for ANN search.

\subsection{Objectives}
The primary objectives of this VnV plan are:
\begin{itemize}
  \item Correctness: Verify that the system correctly implements the mathematical models for training (e.g., MSE loss, gradient descent) and inference (e.g., ANN search, dot product ranking).
  \item Accuracy: Validate that the system achieves acceptable prediction accuracy on a held-out test set.
  \item Scalability: Demonstrate that the system can handle a large number of users or items with reasonable latency.
\end{itemize}
\noindent Out-of-Scope Objectives

\begin{itemize}
  \item External Library Verification: Libraries such as PyTorch and FAISS are assumed to be correct and are not verified as part of this plan.
\end{itemize}

\subsection{Challenge Level and Extras}
This is a non-research project. The extra component of this project will be a user manual.

\subsection{Relevant Documentation}

\citet{Yinying2025RecSys} Software requirements specification for this project.

\section{Plan}

The VnV plan starts with an introduction to the verification and validation team, followed by verification plans for the SRS and design. Next, it covers verification plans for the VnV Plan and implementation. Finally, it includes sections on automated testing and verification tools as well as the software validation plan .

\subsection{Verification and Validation Team}\label{VnVT}

\begin{table}[h]
  \centering
  \resizebox{\textwidth}{!}{ %
      \begin{tabular}{ |l|l|p{2cm}|p{5cm}| } 
  \hline        
  
     Name & Document & Role & Description \\
  \hline
    Yinying Huo & All & Author & Prepare all documentation, develop the software, and validate the implementation accoridng to the VnV plan. \\ \hline
    Dr. Spencer Smith & All & Instructor/ Reviewer & Review all the documents.  \\ \hline
    Yuanqi Xue & All & Domain Expert & Review all the documents.  \\ \hline     	  
  \end{tabular} %
  }
  \caption{Verification and Validation Team}
  \label{Table:VnVT}
  \end{table}
  
\subsection{SRS Verification Plan}

The Software Requirements Specification (SRS) will be reviewed by domain expert Yuanqi Xue and Dr. Smith. Feedback from reviewers will be provided on GitHub, and the author will need to address it.

\noindent There is a \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/SRS-Checklist.pdf}{SRS checklist} designed by Dr. Spencer Smith available to use.

\subsection{Design Verification Plan}

The design verification, including the Module Guide (MG) and Module Interface Specification (MIS), will be reviewed by domain expert Yuanqi Xue and Dr. Smith. Feedback from reviewers will be provided on GitHub, and the author will need to address it.

\noindent Dr. Spencer Smith has created a \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/MG-Checklist.pdf}{MG checklist} and \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/MIS-Checklist.pdf}{MSI checklist}, both of which are available for use.

\subsection{Verification and Validation Plan Verification Plan}

The Verification and Validation (VnV) Plan will be reviewed by domain expert Yuanqi Xue and Dr. Smith. Feedback from reviewers will be provided on GitHub, and the author will need to address it.

\noindent  There is a \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/VnV-Checklist.pdf}{VnV checklist} designed by Dr. Spencer Smith available to use.
%

\subsection{Implementation Verification Plan}

The implementation will be verified by testing both the functional and non-functional requirements outlined in section \ref{SystemTest}. Unit tests, as described in section \ref{UnitTest}, will also be performed. Additionally, a code walkthrough will be conducted with the class during the final presentation.

\subsection{Automated Testing and Verification Tools}
The following tools will be used for automated testing and verification:
\begin{itemize}
\item \textbf{Unit Testing}:
\begin{itemize}
\item \textbf{Pytest}: For testing individual components (e.g., embedding functions, ANN search, ranking logic).
\end{itemize}
\item \textbf{Continuous Integration (CI)}:
\begin{itemize}
\item \textbf{GitHub Actions}: To automate testing and deployment workflows.
\item \textbf{CML (Continuous Machine Learning)}: Automatically generates performance reports and emails metrics when changes are pushed to GitHub.
\item The CI workflow will run all unit tests.
\end{itemize}
\end{itemize}

\subsection{Software Validation Plan}

The system will be validated using a 20\% testing dataset split from the original dataset. This dataset will be used to evaluate the performance of the two embedding functions (user and item towers) by measuring metrics such as recall and precision.

\section{System Tests}\label{SystemTest}

This section covers the system tests that will be applied to both the functional and non-functional requirements.

\subsection{Tests for Functional Requirements}

The functional requirements are tested in the following areas: input validation, ranking consistency, and output correctness. These tests ensure the system behaves as expected under various conditions.

\subsubsection{Area of Testing 3: End-to-End Recommendation}

The following test will ensure that there is no missing data in the training and testing datasets, and that all input constraints are satisfied as specified in the data constraints table of the \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{SRS}.
		
\paragraph{Test for Full Recommendation Pipeline}  

\begin{enumerate}  
  \item{test-id1\\}  
  Type: Automatic 

  Initial State: Untrained system. 

  Input/Condition: Full training dataset.  

  Output/Result: A ranked list of recommendations for test users.

  Test Case Derivation: Validates the entire workflow (training → embedding → ANN search → ranking).  

  How test will be performed:  

  - Train the model on training dataset. 

  - Generate recommendations for test users. 

  - Verify output.  
\end{enumerate}  

% \begin{enumerate}

%   \item{test-id1\\}
%   Control: Automatic\\
%   Initial State: Before training the embedding functions.\\
%   Input: Dataset\\
%   Output: A verified dataset where each user-item pair in the verified dataset has an associated reward and no missing values.\\
%   Test Case Derivation: Ensures the system handles valid inputs as specified in \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{SRS}. \\
%   How test will be performed: Automated test using Pytest.
					
% \end{enumerate}


% \paragraph{Test for performance}

% \begin{enumerate}
%   \item{test-id2\\}

%   Control: Automatic\\
%   Initial State: After training of the embedding functions\\
%   Input: The testing dataset and embedding functions\\
%   Output: The performance of the user embedding and item embedding functions is acceptable (accuracy $>$ 80\%)\\
%   Test Case Derivation: Ensures the correctness of the output as specified in \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{SRS}.\\
%   How the test will be performed: This test will be performed automatically using GitHub Actions once the training is finished.

% \item{test-id3\\}

% Control: Automatic\\
% Initial State: System initialized with pre-computed item embeddings and trained ANN index.\\
% Input:  user embeddings\\
% Output: The ranking for the top 5 items should generally be the same for identical user embeddings. Non-consistent rankings are allowed (up to 5\%) due to the inherent randomness in machine learning.\\
% Test Case Derivation: Ensures that identical user embeddings produce identical item rankings, as stated in \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{SRS}.\\
% How the test will be performed: This test will be performed automatically using GitHub Actions. It will use the user feature 100 times to generate 100 lists of recommended items.\\

% \end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

\subsubsection{Reliability}

The reliability of the software is tested through the tests for
functional requirements in section 4.1 and 5.2 .

\subsubsection{Portability}

\begin{enumerate}

\item{test-id2\\}

Type: Manual
					
Initial State: None
					
Input/Condition: None
					
Output/Result: The project should pass all the tests for functional requirements and run without any errors.

How test will be performed: Potential users will install the project on their computers (Windows, macOS, or Linux) and execute a sample workflow. 
\end{enumerate}


\subsubsection{Scalability}

\begin{enumerate}

\item{test-id3\\}

Type: Manual

Initial State: System initialized with a model trained on 70\% of the dataset.

Input/Condition: Remaining 30\% of the dataset added as new data.

Output/Result: Model updates complete within 1 minute.

How test will be performed: Because there is no external data for updates, the system will first train the model on 70\% of the training set. After training is done and all the functional requirements have passed, the remaining 30\% will be added as new data to simulate incremental updates.


\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
    & test-id1& test-id2& test-id3 \\
  \hline
  R1        & X&   & \\ \hline
  R2        & X& & \\ \hline
  R3        & X& & X\\ \hline
  NFR1      &     & & X\\ \hline
  NFR2      &X    & & \\ \hline
  NFR3      &     &X& \\ \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between Test Cases and Requirements}
  \label{Table:A_trace}
  \end{table}

\section{Unit Test Description}\label{UnitTest}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.


\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\end{document}