\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}
\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Feb 24, 2025 & 1.0 & First draft - Unit tests will be added after the MIS has been completed..\\
Feb 25, 2025 & 1.1 & Minor update after VnV presentation\\
\bottomrule
\end{tabularx}

~\\

\newpage

\tableofcontents

\listoftables

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  R & Requirement\\
  NFR & Nonfunctional Requirement\\
  TTE & Two Tower Embedding\\
  RecSys & Recommendation System\\
  ANN & Approximate Nearest Neighbor\\
  SRS & Software Requirements Specification\\
  FAISS & Facebook AI Similarity Search\\
  DNN & Deep Neural Network\\
  \bottomrule
\end{tabular}\\

\newpage

\pagenumbering{arabic}
\nocite{*}
This document outlines the verification and validation plan for the Two-Tower Embeddings Recommendation System (TTE RecSys) to ensure compliance with the requirements and objectives specified in the \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{Software Requirements Specification (SRS)}. It is structured to first present general information and verification strategies, followed by detailed descriptions of system and unit testing for both functional and non-functional requirements.

\section{General Information}

\subsection{Summary}

The software under test is the Two-Tower Embedding Recommendation System, which generates personalized recommendations using user and item embeddings. The system consists of two main components:
\begin{itemize}
  \item Training Phase: Learns user and item embedding functions using a deep neural network architecture, optimized via Stochastic Gradient Descent (SGD).
  \item Inference Phase: Retrieves candidate items using Approximate Nearest Neighbor (ANN) search and ranks them by dot product similarity.
\end{itemize}

The system is implemented in Python, leveraging libraries such as PyTorch for model training and FAISS for ANN search.

\subsection{Objectives}
The primary objectives of this VnV plan are:
\begin{itemize}
  \item Correctness: Verify that the system correctly implements the mathematical models for training (e.g., MSE loss) and inference (e.g., ANN search, dot product ranking).
  \item Accuracy: Validate that the system achieves acceptable prediction accuracy on a held-out test set.
  \item Scalability: Demonstrate that the system can support incremental update when new data available.
\end{itemize}
\noindent Out-of-Scope Objectives

\begin{itemize}
  \item External Library Verification: Libraries such as PyTorch and FAISS are assumed to be correct and are not verified as part of this plan.
\end{itemize}

\subsection{Challenge Level and Extras}
This is a non-research project. The extra component of this project will be a user manual.

\subsection{Relevant Documentation}
The following documents are available for this project:
\href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{Software Requirements Specification}, \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Design/SoftArchitecture/MG.pdf}{Module Guide}, and \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{Module Interface Specification} 

\section{Plan}

The VnV plan starts with an introduction to the verification and validation team, followed by verification plans for the SRS and design. Next, it covers verification plans for the VnV Plan and implementation. Finally, it includes sections on automated testing and verification tools as well as the software validation plan .

\subsection{Verification and Validation Team}\label{VnVT}

\begin{table}[h]
  \centering
  \resizebox{\textwidth}{!}{ %
      \begin{tabular}{ |l|l|p{2cm}|p{5cm}| } 
  \hline        
  
     Name & Document & Role & Description \\
  \hline
    Yinying Huo & All & Author & Prepare all documentation, develop the software, and validate the implementation accoridng to the VnV plan. \\ \hline
    Dr. Spencer Smith & All & Instructor/ Reviewer & Review all the documents.  \\ \hline
    Yuanqi Xue & All & Domain Expert & Review all the documents.  \\ \hline     	  
  \end{tabular} %
  }
  \caption{Verification and Validation Team}
  \label{Table:VnVT}
  \end{table}
  
\subsection{SRS Verification Plan}

The Software Requirements Specification (SRS) will be reviewed by domain expert Yuanqi Xue and Dr. Smith. Feedback from reviewers will be provided on GitHub, and the author will need to address it.

\noindent There is a \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/SRS-Checklist.pdf}{SRS checklist} designed by Dr. Spencer Smith available to use.

\subsection{Design Verification Plan}

The design verification, including the Module Guide (MG) and Module Interface Specification (MIS), will be reviewed by domain expert Yuanqi Xue and Dr. Smith. Feedback from reviewers will be provided on GitHub, and the author will need to address it.

\noindent Dr. Spencer Smith has created a \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/MG-Checklist.pdf}{MG checklist} and \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/MIS-Checklist.pdf}{MIS checklist}, both of which are available for use.

\subsection{Verification and Validation Plan Verification Plan}

The Verification and Validation (VnV) Plan will be reviewed by domain expert Yuanqi Xue and Dr. Smith. Feedback from reviewers will be provided on GitHub, and the author will need to address it.

\noindent  There is a \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/VnV-Checklist.pdf}{VnV checklist} designed by Dr. Spencer Smith available to use.
%

\subsection{Implementation Verification Plan}

The implementation will be verified by testing both the functional and non-functional requirements outlined in section \ref{SystemTest}. Unit tests, as described in section \ref{UnitTest}, will also be performed. Additionally, a code walkthrough will be conducted with the class during the final presentation.

\subsection{Automated Testing and Verification Tools}
All system tests and unit tests will be performed using Python scripts.  
GitHub Actions is used for continuous integration, and the workflow will run all unit tests.
\subsection{Software Validation Plan}

The software validation plan is beyond the scope of \progname, as it requires additional time and data that are not available within the scope of the project.

\section{System Tests}\label{SystemTest}

This section covers the system tests that will be applied to both the functional and non-functional requirements.

\subsection{Tests for Functional Requirements}

The functional requirements are tested in the following areas: input validation, model training convergence, and model storage. These tests ensure that the system behaves as expected under various conditions.

\subsubsection{Area of Testing 1: Dataset}

\begin{enumerate}

  \item{test-id1\\}
  Control: Automatic\\
  Initial State: Before training the DNN.\\
  Input: Dataset\\
  Output: A verified training dataset where each user-item pair has an associated reward.\\
  Test Case Derivation: Ensures that the system receives a valid dataset for training, as specified in the \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{SRS}.

  How test will be performed: This test will be performed automatically using GitHub Actions every time before training the DNN.
					
\end{enumerate}


\subsubsection{Area of Testing 2: Model Training Convergence}

\begin{enumerate}
  \item{test-id2\\}

  Control: Automatic\\
  Initial State: After training of the DNN\\
  Input: The loss record during training\\
  Output: A boolean value, ``True'' if the loss of the DNN decreases over iterations, and ``False'' otherwise.\\
  Test Case Derivation: Ensures the correctness of the output as specified in \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{SRS}.\\
  How the test will be performed: This test will be performed automatically using GitHub Actions once training is complete.

\end{enumerate}

\subsubsection{Area of Testing 3: Model Storage}

\begin{enumerate}
  \item{test-id3:\\}
  
  Control: Automatic\\
  Initial State: After model training is complete\\
  Input: Path to the model and pre-computed item embeddings\\
  Output: bA boolean variable -- ``True'' iif the model and pre-computed item embeddings are stored in the specified location, and ``False'' otherwise\\
  Test Case Derivation: Ensures R3 (model storage) is properly implemented\\
  How test will be performed: The test will be performed automatically using GitHub Actions once model training convergence is complete.
\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

\subsubsection{Reliability}

The reliability of the software is tested through the tests for
functional requirements in section 4.1 and 5.2 .

\subsubsection{Portability}

\begin{enumerate}

\item{test-id4\\}

Type: Automatic and manual
					
Initial State: None
					
Input/Condition: None
					
Output/Result: The results of all automatic tests and feedback from users.

How test will be performed: All automatic tests will be conducted during the continuous integration workflow. Potential users will install the project on their computers (Windows, macOS, or Linux) and follow the instructions in \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/README.md}{README.md} to run the software. 
\end{enumerate}


\subsubsection{Usability}

\begin{enumerate}

\item{test-id5\\}

Type: Manual

Initial State: The software is setup and ready to use.

Input/Condition: None

Output/Result: Survey result from the user

How test will be performed: The user will be ask to filfiled the servey after using this software. The survey can be find at appendix \ref{US}.


\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

The table \ref{Table:S_trace}
 shows the traceability between test cases and requirements

\begin{table}[h!]\label{Table:S_trace}
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
    & test-id1& test-id2& test-id3&test-id4&test-id5 \\
  \hline
  R1        & X&  & & &  \\ \hline
  R2        & & X& & &  \\ \hline
  R3        & & & X& & \\ \hline
  R4        & & X& & & \\ \hline
  R5        & &X & & & \\ \hline
  R6        & &X & & & X\\ \hline
  NFR1      & & & & &  X\\ \hline
  NFR2      &X & X& X& & \\ \hline
  NFR3      & & & &X & \\ \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between Test Cases and Requirements}
  \label{Table:A_trace}
  \end{table}

\section{Unit Test Description}\label{UnitTest}

The unit tests for this system will follow a hierarchical approach based on the module decomposition in the Module Guide. The testing philosophy focuses on:

1. Black-box testing of module interfaces according to their specifications
2. White-box testing for complex algorithms and edge cases
3. Mock objects for isolating modules from their dependencies

\subsection{Unit Testing Scope}

The unit testing will focus on the four core modules of the system: Data Processing, Vector Operations, Neural Network Architecture, and Recommendation. Other modules will be tested indirectly through system tests or as part of the testing of these core modules. External libraries (PyTorch, FAISS) are considered outside the scope of unit testing.

\subsection{Tests for Functional Requirements}


\subsubsection{Data Processing Module (M2)}

\begin{enumerate}
  \item{test-M2-1: Dataset Validation Test\\}
  Type: Automatic, Functional
            
  Initial State: None
            
  Input: Path to test dataset
            
  Output: Boolean value True if the dataset meets all validation criteria

  Test Case Derivation: R1 requires the system to accept valid input data.
  
  How test will be performed: 
  Load the dataset and run validation with the module's validate\_data method.

  \item{test-M2-2 Data Loading Test\\}
  Type: Automatic, Functional

  Initial State: None

  Input: Path to CSV file

  Output: Dataset containing loaded data

  Test Case Derivation: The system needs to load data correctly.

  How test will be performed: Load data from a test CSV and verify the shape and columns match expected values.

  \item{test-M2-3 Data Preprocessing Test\\}
  Type: Automatic, Functional

  Initial State: None

  Input: Dataset

  Output: Processed dataset with derived features

  Test Case Derivation: R1 requires the system to preprocess data.

  How test will be performed: Process sample data and verify all expected derived features are created correctly.

  \item{test-M2-4: Missing Value Handling Test\\}
  
  Type: Automatic, Functional

  Initial State: None

  Input: Dataset with missing values

  Output: Processed Dataset without missing values

  Test Case Derivation: The system must handle incomplete data gracefully.

  How test will be performed: Create a DataFrame with missing values, process it, and verify no NaN values remain.

  \item{test-M2-5: Training Data Creation Test\\}
  Type: Automatic, Functional

  Initial State: None

  Input: dataset

  Output: Dictionary containing training data arrays

  Test Case Derivation: The system needs to convert dataset to training format.

  How test will be performed: Create training data from processed sample and verify the structure and shapes of the output.
\end{enumerate}


\subsubsection{Embedding Generation Module (M4)}
\begin{enumerate}

  \item{test-M4-1: User Embedding Generation Test\\}
  Type: Automatic, Functional
            
  Initial State: Initialized EmbeddingGenerator
            
  Input: A random user data from the dataset
            
  Output: User embedding vector of the expected dimension.
  
  Test Case Derivation: R4 requires the system to generate embeddings based on user features.
  
  How test will be performed: Generate an embedding for a user from the dataset and verify its dimension and normalization.

  \item{test-M4-2: Item Embedding Generation Test\\}
  
  Type: Automatic, Functional

  Initial State: Initialized EmbeddingGenerator

  Input: A random item data from the dataset

  Output: Item embedding vector of the expected dimension.

  Test Case Derivation: R4 requires the system to generate embeddings based on item features.

  How test will be performed: Generate an embedding for an item from the dataset and verify its dimension and normalization.

  \item{test-M4-3: Embedding Generator Initialization Test}
  Type: Automatic, Functional

  Initial State: None

  Input: User and item models

  Output: Initialized EmbeddingGenerator

  Test Case Derivation: The system must initialize embedding generators with proper models.

  How test will be performed: Initialize the generator with models and verify models are properly set.
  \end{enumerate}


\subsubsection{Neural Network Architecture Module (M6)}

\begin{enumerate}

\item{test-M6-1: Network Architecture Test\\}
Type: Automatic, Functional
					
Initial State: None
					
Input: Configuration parameters for input dimension, hidden layers, and embedding dimension
					
Output: Neural network model with the specified architecture

Test Case Derivation: R2 requires the system to create models for embedding generation.

How test will be performed: Create a model with the architecture and verify the layer structure matches the specification.

\item{test-M6-2: Forward Pass Test\\}

Type: Automatic, Functional

Initial State: Initialized neural network

Input: Batch of user/item features

Output: Embeddings with expected shape and normalization

Test Case Derivation: The network must produce properly normalized embeddings.

How test will be performed: Run a forward pass on sample data and verify output dimensions and normalization.

\item{test-M6-3: Network Initialization Test}

Type: Automatic, Functional

Initial State: None

Input: Configuration parameters

Output: Network with properly initialized weights

Test Case Derivation: Proper weight initialization is essential for training convergence.

How test will be performed: Create a network and verify weights are initialized according to the specified scheme.

\item{test-M6-4: Batch Processing Test\\}

Type: Automatic, Functional

Initial State: Initialized neural network

Input: Batch of features and individual features

Output: Identical embeddings for batch and individual processing

Test Case Derivation: Batch processing should behave the same as individual processing.

How test will be performed: Compare embeddings generated by batch processing with those from individual processing.

\end{enumerate}
\subsubsection{ANN Search Module (M7)}

\begin{enumerate}

\item{test-M7-1: Exact Match Search Test\\}
Type: Automatic, Functional
					
Initial State: None
					
Input: Array of item embeddings from the production model and a query embedding
					
Output: Array of (item\_id, similarity\_score) tuples with the expected length

Test Case Derivation: R6 requires efficient retrieval of nearest items.

How test will be performed: Build an index with item embeddings, search for an exact match, and verify the result.

\item{test-M7-2: Approximate Match Search Test\\}
Type: Automatic, Functional

Initial State: None

Input: Array of item embeddings and a query embedding that is similar to one of the items

Output: Array of (item\_id, similarity\_score) tuples where the top results include the similar item

Test Case Derivation: R6 requires the system to find similar items.

How test will be performed: Build an index, search with a similar query, and verify the results include the expected item.

\item{test-M7-3: Index Save/Load Test}
Type: Automatic, Functional

Initial State: None

Input: ANN index and path to save/load

Output: Loaded index matching the saved index

Test Case Derivation: R3 requires model storage and loading.

How test will be performed: Save an index, load it back, and verify the loaded index produces the same search results.

\item{test-M7-4: Multiple Results Test\\}
Type: Automatic, Functional

Initial State: None

Input: Array of item embeddings, query embedding, and number of results

Output: Array of (item\_id, similarity\_score) tuples with the 
\end{enumerate}

\subsubsection{Vector Operations Module (M8)}

\begin{enumerate}

\item{test-M8-1\\}
Type: Automatic, Functional
					
Initial State: None
					
Input: Two embedding vectors
					
Output: Scalar dot product value

Test Case Derivation: The system relies on dot product for similarity calculation.

How test will be performed: Calculate the dot product of two vectors and compare it with the expected value.

\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

Unit testing the nonfunctional requirements is beyond the scope. 

\subsection{Traceability Between Test Cases and Modules}
The table \ref{Table:Unit} shows the traceability between test cases and modules.


\begin{table}[h!]\label{Table:Unit}
  \centering
  \begin{tabular}{|l|c|c|c|c|c|c|c|c|}
  \hline
  \textbf{Test ID} & \textbf{M1} & \textbf{M2} & \textbf{M3} & \textbf{M4} & \textbf{M5} & \textbf{M6} & \textbf{M7} & \textbf{M8} \\
  \hline
  test-M2-1 &   & X &   &   &   &   &   &   \\
  test-M3-1 &   &   & X &   &   &   &   &   \\
  test-M4-1 &   &   &   & X &   &   &   &   \\
  test-M5-1 &   &   &   &   & X &   &   &   \\
  test-M6-1 &   &   &   &   &   & X &   &   \\
  test-M7-1 &   &   &   &   &   &  &  X &   \\
  test-M8-1 &   &   &   &   &   &   &   & X \\
  \hline
  \end{tabular}
  \caption{Traceability Matrix Between Test Cases and Modules}
  \end{table}

  \bibliographystyle {plainnat}
  \bibliography {../../refs/References}

\newpage
\section{Appendix}


\subsection{Usability Survey}\label{US}
\begin{itemize}
\item On a scale from 1 to 5, with 5 being the most satisfied, how would you rate your overall experience using the software?
\item Were the installation and setup process straightforward? If not, what difficulties did you encounter?
\item Did you find the user interface intuitive and easy to navigate? If not, what improvements would you suggest?
\item Did the software meet your expectations? If not, what features or improvements would you like to see?
\end{itemize}

\end{document}