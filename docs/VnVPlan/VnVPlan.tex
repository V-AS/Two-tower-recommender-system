\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Feb 24, 2025 & 1.0 & First draft - Unit tests will be added after the MIS has been completed..\\
Feb 25, 2025 & 1.1 & Minor update after VnV presentation\\
\bottomrule
\end{tabularx}

~\\

\newpage

\tableofcontents

\listoftables

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  R & Requirement\\
  NFR & Nonfunctional Requirement\\
  TTE & Two Tower Embedding\\
  RecSys & Recommendation System\\
  ANN & Approximate Nearest Neighbor\\
  SRS & Software Requirements Specification\\
  FAISS & Facebook AI Similarity Search\\

  \bottomrule
\end{tabular}\\

\newpage

\pagenumbering{arabic}

This document outlines the verification and validation plan for the Two-Tower Embeddings Recommendation System (TTE RecSys) to ensure compliance with the requirements and objectives specified in the Software Requirements Specification (SRS). It is structured to first present general information and verification strategies, followed by detailed descriptions of system and unit testing for both functional and non-functional requirements.

\section{General Information}

\subsection{Summary}

The software under test is the Two-Tower Embedding Recommendation System, which generates personalized recommendations using user and item embeddings. The system consists of two main components:
\begin{itemize}
  \item Training Phase: Learns user and item embedding functions using a deep neural network architecture, optimized via stochastic gradient descent (SGD).
  \item Inference Phase: Retrieves candidate items using Approximate Nearest Neighbor (ANN) search and ranks them by dot product similarity.
\end{itemize}

The system is implemented in Python, leveraging libraries such as PyTorch for model training and FAISS for ANN search.

\subsection{Objectives}
The primary objectives of this VnV plan are:
\begin{itemize}
  \item Correctness: Verify that the system correctly implements the mathematical models for training (e.g., MSE loss) and inference (e.g., ANN search, dot product ranking).
  \item Accuracy: Validate that the system achieves acceptable prediction accuracy on a held-out test set.
  \item Scalability: Demonstrate that the system can support incremental update when new data available.
\end{itemize}
\noindent Out-of-Scope Objectives

\begin{itemize}
  \item External Library Verification: Libraries such as PyTorch and FAISS are assumed to be correct and are not verified as part of this plan.
\end{itemize}

\subsection{Challenge Level and Extras}
This is a non-research project. The extra component of this project will be a user manual.

\subsection{Relevant Documentation}

\citet{Yinying2025RecSys} Software requirements specification for this project.

\section{Plan}

The VnV plan starts with an introduction to the verification and validation team, followed by verification plans for the SRS and design. Next, it covers verification plans for the VnV Plan and implementation. Finally, it includes sections on automated testing and verification tools as well as the software validation plan .

\subsection{Verification and Validation Team}\label{VnVT}

\begin{table}[h]
  \centering
  \resizebox{\textwidth}{!}{ %
      \begin{tabular}{ |l|l|p{2cm}|p{5cm}| } 
  \hline        
  
     Name & Document & Role & Description \\
  \hline
    Yinying Huo & All & Author & Prepare all documentation, develop the software, and validate the implementation accoridng to the VnV plan. \\ \hline
    Dr. Spencer Smith & All & Instructor/ Reviewer & Review all the documents.  \\ \hline
    Yuanqi Xue & All & Domain Expert & Review all the documents.  \\ \hline     	  
  \end{tabular} %
  }
  \caption{Verification and Validation Team}
  \label{Table:VnVT}
  \end{table}
  
\subsection{SRS Verification Plan}

The Software Requirements Specification (SRS) will be reviewed by domain expert Yuanqi Xue and Dr. Smith. Feedback from reviewers will be provided on GitHub, and the author will need to address it.

\noindent There is a \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/SRS-Checklist.pdf}{SRS checklist} designed by Dr. Spencer Smith available to use.

\subsection{Design Verification Plan}

The design verification, including the Module Guide (MG) and Module Interface Specification (MIS), will be reviewed by domain expert Yuanqi Xue and Dr. Smith. Feedback from reviewers will be provided on GitHub, and the author will need to address it.

\noindent Dr. Spencer Smith has created a \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/MG-Checklist.pdf}{MG checklist} and \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/MIS-Checklist.pdf}{MSI checklist}, both of which are available for use.

\subsection{Verification and Validation Plan Verification Plan}

The Verification and Validation (VnV) Plan will be reviewed by domain expert Yuanqi Xue and Dr. Smith. Feedback from reviewers will be provided on GitHub, and the author will need to address it.

\noindent  There is a \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/Checklists/VnV-Checklist.pdf}{VnV checklist} designed by Dr. Spencer Smith available to use.
%

\subsection{Implementation Verification Plan}

The implementation will be verified by testing both the functional and non-functional requirements outlined in section \ref{SystemTest}. Unit tests, as described in section \ref{UnitTest}, will also be performed. Additionally, a code walkthrough will be conducted with the class during the final presentation.

\subsection{Automated Testing and Verification Tools}
The following tools will be used for automated testing and verification:
\begin{itemize}
\item \textbf{Unit Testing}:
\begin{itemize}
\item \textbf{Pytest}: For testing individual components (e.g., embedding functions, ANN search, ranking logic).
\end{itemize}
\item \textbf{Continuous Integration (CI)}:
\begin{itemize}
\item \textbf{GitHub Actions}: To automate testing and deployment workflows.
\item \textbf{CML (Continuous Machine Learning)}: Automatically generates performance reports and emails metrics when changes are pushed to GitHub.
\item The CI workflow will run all unit tests.
\end{itemize}
\end{itemize}

\subsection{Software Validation Plan}

The system will be validated using a 20\% testing dataset split from the original dataset. This dataset will be used to evaluate the performance of the two embedding functions (user and item towers) by measuring metrics such as recall and precision.

\section{System Tests}\label{SystemTest}

This section covers the system tests that will be applied to both the functional and non-functional requirements.

\subsection{Tests for Functional Requirements}

The functional requirements are tested in the following areas: input validation, ranking consistency, and output correctness. These tests ensure the system behaves as expected under various conditions.

\subsubsection{Area of Testing 1: Dataset}

% The following test verifies that the entire recommendation pipeline works as expected—from data ingestion, through embedding generation, to the delivery of final recommendations.
		
% \paragraph{Test for Full Recommendation Pipeline}  

% \begin{enumerate}  
%   \item{test-id1\\}  
%   Type: Automatic 

%   Initial State: Untrained system. 

%   Input/Condition: Full training dataset.  

%   Output/Result: A ranked list of recommendations for test users.

%   Test Case Derivation: Validates the entire workflow (training → embedding → ANN search → ranking).  

%   How test will be performed:  

%   - Train the model on training dataset. 

%   - Generate recommendations for test users. 

%   - Verify output.  
% \end{enumerate}  

\begin{enumerate}

  \item{test-id1\\}
  Control: Automatic\\
  Initial State: Before training the embedding functions.\\
  Input: Dataset\\
  Output: A verified dataset where each user-item pair in the verified dataset has an associated reward and no missing values.\\
  Test Case Derivation: Ensures that the system handles valid inputs as specified in the \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{SRS}, which also includes the training dataset used for this project.

  How test will be performed: Automated test using GitHub Actions.
					
\end{enumerate}


\subsubsection{Test for performance}

\begin{enumerate}
  \item{test-id2\\}

  Control: Automatic\\
  Initial State: After training of the embedding functions\\
  Input: The testing dataset and embedding functions\\
  Output: The performance of the user embedding and item embedding functions is acceptable (accuracy $>$ 80\%)\\
  Test Case Derivation: Ensures the correctness of the output as specified in \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{SRS}.\\
  How the test will be performed: This test will be performed automatically using GitHub Actions once the training is finished.

\item{test-id3\\}

Control: Automatic\\
Initial State: System initialized with pre-computed item embeddings and trained ANN index.\\
Input:  user embeddings\\
Output: The ranking for the top 5 items should generally be the same for identical user embeddings. Non-consistent rankings are allowed (up to 5\%) due to the inherent randomness in machine learning.\\
Test Case Derivation: Ensures that identical user embeddings produce identical item rankings, as stated in \href{https://github.com/V-AS/Two-tower-recommender-system/blob/main/docs/SRS/SRS.pdf}{SRS}.\\
How the test will be performed: This test will be performed automatically using GitHub Actions. It will generate 100 lists of recommended items for a random user.\\

\end{enumerate}

\subsubsection{Area of Testing 3: Model Storage}

\begin{enumerate}
  \item{test-id4:\\}
  
  Control: Automatic\\
  Initial State: After model training is complete\\
  Input: Trained model to be saved\\
  Output: Model successfully stored and can be retrieved with identical parameters\\
Test Case Derivation: Ensures R3 (model storage) is properly implemented
How test will be performed: Automated test that saves the model, loads it back, and verifies parameter integrity
\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

\subsubsection{Reliability}

The reliability of the software is tested through the tests for
functional requirements in section 4.1 and 5.2 .

\subsubsection{Portability}

\begin{enumerate}

\item{test-id5\\}

Type: Manual
					
Initial State: None
					
Input/Condition: None
					
Output/Result: The project should pass all the tests for functional requirements and run without any errors.

How test will be performed: Potential users will install the project on their computers (Windows, macOS, or Linux) and execute a sample workflow. 
\end{enumerate}


\subsubsection{Scalability}

\begin{enumerate}

\item{test-id6\\}

Type: Manual

Initial State: System initialized with a model trained on 70\% of the dataset.

Input/Condition: Remaining 30\% of the dataset added as new data.

Output/Result: Model updates complete within 1 minute.

How test will be performed: Because there is no external data for updates, the system will first train the model on 70\% of the training set. After training is done and all the functional requirements have passed, the remaining 30\% will be added as new data to simulate incremental updates.


\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

The table \ref{Table:S_trace}
 shows the traceability between test cases and requirements

\begin{table}[h!]\label{Table:S_trace}
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|}
  \hline
    & test-id1& test-id2& test-id3&test-id4&test-id5&test-id6 \\
  \hline
  R1        & X&  & & & & \\ \hline
  R2        & & X& & & & \\ \hline
  R3        & & & &X& &\\ \hline
  R4        & & & X& & &\\ \hline
  R5        & &X & X& & &\\ \hline
  R6        & &X & X& & &\\ \hline
  NFR1      & & & & & &X\\ \hline
  NFR2      &X & X& X& X& &\\ \hline
  NFR3      & & & & &X&\\ \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between Test Cases and Requirements}
  \label{Table:A_trace}
  \end{table}

\section{Unit Test Description}\label{UnitTest}

The unit tests for this system will follow a hierarchical approach based on the module decomposition in the Module Guide. The testing philosophy focuses on:

1. Black-box testing of module interfaces according to their specifications
2. White-box testing for complex algorithms and edge cases
3. Mock objects for isolating modules from their dependencies

\subsection{Unit Testing Scope}
All modules developed for this project will be tested. External libraries (PyTorch, FAISS) are considered outside the scope of unit testing.

\subsection{Tests for Functional Requirements}

\subsubsection{System Interface Module (M1)}

\begin{enumerate}

  \item{test-M1-1\\}
  Type: Automatic, Functional
            
  Initial State: Empty file system storage location
  
  Input: A trained model object with known parameters
            
  Output: Boolean value 'True' indicating successful save operation, and file exists at the specified location with correct format
  
  Test Case Derivation: R3 requires the system to store trained embedding functions. This test verifies the save model functionality works correctly by checking both the return value and the actual file creation.
  
  How test will be performed: Create a small test model, save it to a temporary location, verify the return value is True, and check that the file exists and has non-zero size.
  
  \item{test-M1-2\\}
  Type: Automatic, Functional
            
  Initial State: File system with a previously saved model
            
  Output: A model object with identical parameters to the one that was saved
  
  Test Case Derivation: R3 requires stored models to be retrievable. This test verifies that a model can be loaded without corruption by comparing parameters before and after the save-load cycle.
  
  How test will be performed: Save a model with known architecture and parameter values, load it back, and verify the architecture and parameters match the original.
  
  \item{test-M1-3\\}
  Type: Automatic, Functional, Exception Handling
            
  Initial State: File system with no access permissions
            
  Input: A model to be saved to a protected location
            
  Output: IOError exception with appropriate error message
  
  Test Case Derivation: The system should handle file system errors gracefully. This test verifies that the appropriate exception is raised when file operations fail.
  
  How test will be performed: Attempt to save a model to a location without write permissions and verify the correct exception is raised.
  
  \item{test-M1-4\\}
  Type: Automatic, Functional
            
  Initial State: File system with a previously saved batch of item embeddings
            
  Input: Path to the saved embeddings
            
  Output: Array of embeddings identical to the ones that were saved
  
  Test Case Derivation: R3 requires precomputed item embeddings to be stored and retrieved. This test verifies that embeddings can be loaded correctly.
  
  How test will be performed: Save a batch of embeddings with known values, load them back, and verify the values match the original.
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.


\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\end{document}